"""Configuration settings for the GH Archive Pipeline"""

import os
from datetime import datetime, timedelta
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

class Config:
    # GH Archive settings
    GH_ARCHIVE_BASE_URL = "https://data.gharchive.org/"
    
    # Date range for processing (default: last 24 hours)
    ARCHIVE_START_DATE = "2024-01-01"  # Format: YYYY-MM-DD
    ARCHIVE_END_DATE = "2024-01-01"    # Format: YYYY-MM-DD
    
    # Target languages and file extensions
    TARGET_LANGUAGES = ["Python", "Java", "JavaScript"]
    TARGET_EXTENSIONS = [".py", ".java", ".js"]
    
    # Repository filtering
    MIN_STARS = 10
    MAX_REPOS = 100  # Keep small for testing
    
    # Creation date filtering (optional)
    MIN_CREATION_DATE = "2024-01-01"  # Only include repos created after this date (YYYY-MM-DD)
    
    # LLM detection keywords
    LLM_KEYWORDS = [
        "chatgpt", "claude", "llama", "mistral", "gpt-4", "gemini", "copilot",
        "openai", "anthropic", "huggingface", "ai-generated", "generated by",
        "ai-assisted", "machine-generated"
    ]
    
    # Output settings
    OUTPUT_DIR = "output"
    EXTRACTED_FILES_DIR = os.path.join(OUTPUT_DIR, "extracted_files")
    LOGS_DIR = os.path.join(OUTPUT_DIR, "logs")
    
    # GitHub API - now loads from .env file
    GITHUB_TOKEN = os.getenv("GITHUB_TOKEN", "")
    
    @classmethod
    def get_archive_url(cls, date_str, hour):
        """Generate URL for specific GH Archive file"""
        return f"{cls.GH_ARCHIVE_BASE_URL}{date_str}-{hour:02d}.json.gz"
    
    @classmethod
    def create_output_dirs(cls):
        """Create necessary output directories"""
        os.makedirs(cls.OUTPUT_DIR, exist_ok=True)
        os.makedirs(cls.EXTRACTED_FILES_DIR, exist_ok=True)
        os.makedirs(cls.LOGS_DIR, exist_ok=True) 