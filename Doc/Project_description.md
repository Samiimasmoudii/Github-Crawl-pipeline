# Code Extraction Pipeline - Detailed Project Description (GH Archive Version)

## **Project Overview**

A modular, configurable pipeline designed to extract clean, non-LLM-generated source code files from GitHub repositories using GH Archive data. The system ensures data integrity for machine learning training purposes by filtering out code that may have been generated by or trained on Large Language Models.

## **Core Objectives**

### **Primary Goal**
Extract high-quality source code files from GitHub repositories that were committed after a specified date threshold, ensuring the code predates LLM training data to avoid contamination.

### **Quality Assurance Goals**
- **Temporal Filtering**: Only collect code committed after user-specified dates
- **LLM Contamination Prevention**: Exclude code that shows evidence of LLM generation
- **Quality Filtering**: Target repositories with minimum community validation (10+ stars)
- **Format Preservation**: Maintain original file extensions and structure

## **Technical Specifications**

### **Input Parameters**

Configuration:
  - target_date: "2023-12-31"  # Cutoff date for commits
  - languages: ["Python", "Java", "C++"]  # Programming languages
  - file_extensions: [".py", ".java", ".cpp"]  # Target file types
  - min_stars: 10  # Minimum repository stars
  - max_repos: 1000  # Maximum repositories to process
  - archive_start_date: "2024-01-01"  # Start date for GH Archive processing
  - archive_end_date: "2024-12-31"    # End date for GH Archive processing

### **Data Sources**

#### **Primary Source: GH Archive**
- **URL**: https://www.gharchive.org/
- **Format**: Hourly JSON archives (gzip compressed)
- **Coverage**: Public GitHub timeline events since 2011
- **Event Types**: PushEvent, CreateEvent, ForkEvent, WatchEvent, etc.
- **Download Pattern**: `http://data.gharchive.org/YYYY-MM-DD-HH.json.gz`

#### **Secondary Source: GitHub API**
- **Purpose**: File content retrieval and repository metadata
- **Usage**: Only for downloading actual file content after filtering
- **Rate Limiting**: 5000 requests/hour (authenticated)

### **Repository Selection Criteria**
- **Minimum Stars**: 10+ (validated via GitHub API)
- **Language Specificity**: Must match user-specified languages
- **Recent Activity**: Active development within specified date range
- **Size Constraints**: Based on processing capacity
- **Accessibility**: Public repositories only

### **LLM Detection System**

#### **Keyword-Based Detection**
Monitor for mentions of:
- **AI Models**: ChatGPT, Claude, Llama, Mistral, GPT-4, Gemini, Copilot
- **AI Services**: OpenAI, Anthropic, HuggingFace, AI-generated
- **Generation Terms**: "generated by", "AI-assisted", "machine-generated"

#### **Pattern-Based Detection**
- **Comment Patterns**: Typical LLM-generated comment structures
- **Code Patterns**: Common LLM coding signatures
- **Documentation Patterns**: AI-generated documentation styles

#### **Scope of Detection**
- **File Content**: Scan actual source code and comments
- **Commit Messages**: Analyze commit descriptions from GH Archive
- **Repository Metadata**: Check README, issues, descriptions

### **Filtering Strategy**

#### **File-Level Filtering** 
- Analyze each file individually
- Exclude files with LLM indicators
- Preserve clean files from partially contaminated repositories

#### **Repository-Level Filtering** (Optional)
- Skip entire repositories with LLM mentions
- More conservative but potentially wasteful approach

## **Pipeline Architecture**

### **Stage 1: GH Archive Data Collection**
```python
GH Archive Processing:
├── Download hourly JSON archives for date range
├── Decompress and parse JSON events
├── Filter for PushEvent types
├── Extract repository and commit information
└── Store processed events locally
```

### **Stage 2: Repository Discovery**
```python
From GH Archive Events:
├── Extract unique repositories from push events
├── Filter by programming language (from push payloads)
├── Collect repository metadata
├── Validate minimum activity threshold
└── Query GitHub API for star counts and additional metadata
```

### **Stage 3: Temporal Filtering**
```python
For each repository:
├── Filter commits after threshold date (from GH Archive)
├── Identify modified files from push payloads
├── Extract file paths and extensions
└── Skip repositories with no recent commits
```

### **Stage 4: File Extraction**
```python
For each valid commit:
├── Use GitHub API to download specified file types
├── Preserve original file structure (Optional)
├── Track file metadata
└── Skip non-code files (README, CSV, etc.)
```

### **Stage 5: LLM Detection & Scoring**
```python
For Each Repo:
├── Scan commit messages from GH Archive
├── Generate repo-level LLM-score (0-100)
├── Flag suspicious repositories

For each extracted file:
├── Keyword scanning
├── Pattern analysis
├── Generate file-level LLM-score (0-100)
├── Flag suspicious files
└── Log detection reasons
```

### **Stage 6: Output Generation**
```python
Generate two outputs:
├── Raw Files: Original extensions preserved
└── Metadata CSV: Comprehensive tracking including GH Archive sources
```

## **Output Specifications**

### **File Structure**
```
output/
├── extracted_files/
│   ├── repo_1/
│   │   ├── file1.py
│   │   ├── file2.java
│   │   └── file3.cpp
│   ├── repo_2/
│   │   └── ...
│   └── ...
├── metadata.csv
└── processing_logs/
    ├── gharchive_download.log
    ├── repository_discovery.log
    └── file_extraction.log
```

### **CSV Schema**
```csv
file_path,sha,github_url,repo_name,commit_date,author,file_size,language,llm_score,llm_flags,extraction_date,gharchive_source,push_event_id
extracted_files/repo_1/file1.py,a1b2c3d4...,https://github.com/user/repo/blob/main/file1.py,user/repo,2024-01-15,john_doe,2048,Python,5,none,2024-02-01,2024-01-15-14.json.gz,12345678
```

### **LLM Scoring System**
- **Score Range**: 0-100 (lower is better)
- **Threshold**:    
    - Files with score > 50 Will be rejected
    - Files with score > 20 flagged for review

- **Scoring Factors**:
  - Keyword frequency: +10 per mention
  - Pattern matches: +15 per pattern
  - Commit message mentions: +25 (from GH Archive)
  - Repository-level mentions: +5

## **GH Archive Processing Details**

### **Data Download Strategy**
```python
Download Process:
├── Generate date range (hourly intervals)
├── Download compressed JSON files
├── Verify file integrity
├── Extract and parse JSON events
└── Store in local processing directory
```

### **Event Processing**
```python
For each hourly archive:
├── Parse JSON lines
├── Filter for PushEvent types
├── Extract repository information
├── Extract commit details
├── Store relevant file change information
└── Track processing progress
```

### **Storage Optimization**
- **Incremental Processing**: Process archives in chronological order
- **Checkpointing**: Save progress to resume interrupted runs
- **Selective Storage**: Only keep relevant event data
- **Compression**: Compress processed data for storage efficiency

## **Rate Limiting & Error Handling**

### **GH Archive Management**
- **No Rate Limits**: Direct HTTP download from GH Archive
- **Bandwidth Consideration**: Manage download speed to avoid overwhelming servers
- **Error Handling**: Retry failed downloads with exponential backoff
- **Integrity Checks**: Verify JSON parsing and file completeness

### **GitHub API Management**
- **Rate Limit**: 5000 requests/hour (authenticated)
- **Usage**: Only for file content retrieval and repository metadata
- **Handling**: Automatic pause with countdown timer
- **Display**: "API LIMIT REACHED, CONTINUING IN 2 MINS 43 SECS..."
- **Recovery**: Automatic resumption after reset

### **Error Handling**
- **Network Errors**: Retry with exponential backoff
- **Repository Access**: Skip private/deleted repositories
- **File Access**: Log inaccessible files, continue processing
- **Corruption**: Validate file integrity, skip corrupted files
- **Archive Errors**: Skip corrupted archive files, continue with next

## **Modularity & Reusability**

### **Component Structure**
```python
pipeline/
├── config.py           # Configuration management
├── gharchive_client.py # GH Archive download and processing
├── github_client.py    # GitHub API wrapper (for file content)
├── llm_detector.py     # LLM detection logic
├── file_processor.py   # File handling utilities
├── event_processor.py  # GH Archive event processing
├── pipeline.py         # Main orchestration
├── utils.py           # Common utilities
└── main.py            # Entry point
```

### **Configuration Options**
- **Flexible Inputs**: Date ranges, languages, file types
- **Adjustable Thresholds**: Star counts, LLM scores
- **Extensible Detection**: Easy to add new LLM patterns
- **Output Formats**: Configurable output structure
- **Archive Processing**: Configurable batch sizes and parallel processing

## **Quality Assurance Features**

### **Data Integrity**
- **SHA Verification**: Ensure file integrity
- **Duplicate Detection**: Prevent duplicate extractions across archives
- **Metadata Validation**: Verify all required fields
- **Audit Trail**: Complete processing history with GH Archive sources

### **Performance Optimization**
- **Batch Processing**: Process multiple archives simultaneously
- **Caching**: Cache repository metadata and file content
- **Resume Capability**: Restart from last processed archive
- **Progress Tracking**: Real-time processing updates
- **Parallel Processing**: Process multiple archives concurrently

## **Usage Examples**

### **Basic Extraction**
```bash
python main.py --start-date "2024-01-01" --end-date "2024-01-31" --language "Python" --extensions ".py"
```

### **Multi-Language Extraction**
```bash
python main.py --start-date "2023-12-01" --end-date "2024-01-31" --language "Java,Python,C++" --extensions ".java,.py,.cpp" --min-stars 50
```

### **Custom Configuration**
```bash
python main.py --config custom_config.yaml --output-dir /path/to/output --parallel-archives 4
```

### **Resume Processing**
```bash
python main.py --resume --checkpoint-file pipeline_state.json
```

## **Advantages of GH Archive Approach**

### **Benefits**
- **No Rate Limits**: Direct access to historical data
- **Complete Timeline**: Access to all public GitHub events
- **Efficient Discovery**: Bulk processing of repository information
- **Historical Analysis**: Ability to analyze patterns over time
- **Cost Effective**: No API quotas or costs for discovery

### **Considerations**
- **Storage Requirements**: Local storage needed for archive processing
- **Processing Time**: Initial download and processing of archives
- **GitHub API Still Needed**: For file content retrieval
- **Data Freshness**: Archive data has ~1 hour delay
